# TK Title

## Data

### Load profiles

Our ComEd Illinois data consists of kWh load profiles in 30-minute increments for the complete set of households across the entire ComEd service area. To preserve anonymity per rules set by the Illinois Chamber of Commerce, customer data can only be included in a data release from a utility company if it passes a screening process. In this case, a customer’s individual data cannot be released if there are 15 or fewer customers in the given geographic area, or if they represent more than 15% of that area’s load. In our case, the geographic area of interest is the nine-digit Zip+4 postal code.

::: {.callout-note}
## Ameren data was also grabbed for a hypothetical second project with CUB. It's not part of the scope of this round of work--CUB explicitly asked for the ComEd data first and separately
:::

Furthermore, even when individual customers’ usage data is provided, the identification number associated with each customer is not retained month to month. In other words, while the usage data may feature the same customers in September as they did in August, ComEd assigns each customer a new account ID for each month. As a result, we only get consistent household identifiers for a given calendar month.

Since Illinois’s grid peaked in July 2023, for our initial test analysis we choose load profiles for the month of July 2023. We denote a household’s **monthly** 30-minute interval usage series as $L_i$ (for household $i$). For July 2023, each $L_i$ is a 1,488-point time series observed at 30-minute intervals (48 half-hour readings per day over 31 days).

For clustering, however, we work with **household-day observations** derived from these monthly series. Specifically, we partition each $L_i$ into daily 48-point vectors $L_{id}$, where $d\in\{1,\ldots,31\}$ indexes days in July. Each household-day vector $L_{id}$ is then normalized to a daily load-shape vector $S_{id}$ prior to clustering. In our clustering implementation, this normalization is performed row-wise and can be specified as min–max scaling (primary specification), z-score scaling (robustness check), or no additional scaling.

These normalized household-day observations $\{S_{id}\}$ are the inputs to k-means, and all subsequent aggregation and regression in Stage 2 is performed at the household-day level.

::: {.callout-note}
## I am still unsure about the scale of the final analysis. Will we show a full year of data, or a particular month/set of months? Or 12 months as a series of sub-analyses.
:::

### Demographic information

The highest spatial resolution demographic data available for our analysis is the 5-year 2023 US Census Bureau American Community Survey (2023 ACS) and the decennial 2020 Census Supplemental Demographic and Housing Characteristics File (2020 DHS) at the block group level (DHS 2020).

From this, we derive 47 block-group level demographic features across five categories (all sourced from ACS unless otherwise noted):

* Spatial (1 variable): `urban_percent`.

* Economic (7 variables): `median_household_income`, `unemployment_rate`, `pct_in_civilian_labor_force`, `pct_not_in_labor_force`, `pct_income_under_25k`, `pct_income_25k_to_75k`, `pct_income_75k_plus`.

* Housing (24 variables): `pct_owner_occupied`, `pct_renter_occupied`, `pct_heat_utility_gas`, `pct_heat_electric`, `pct_housing_built_2000_plus`, `pct_housing_built_1980_1999`, `old_building_pct`, `pct_structure_single_family_detached`, `pct_structure_single_family_attached`, `pct_structure_multifamily_2_to_4`, `pct_structure_multifamily_5_to_19`, `pct_structure_multifamily_10_plus`, `pct_structure_multifamily_20_plus`, `pct_structure_mobile_home`, `pct_vacant_housing_units`, `pct_home_value_under_150k`, `pct_home_value_150k_to_299k`, `pct_home_value_300k_plus`, `pct_rent_burden_30_plus`, `pct_rent_burden_50_plus`, `pct_owner_cost_burden_30_plus_mortgage`, `pct_owner_cost_burden_50_plus_mortgage`, `pct_owner_overcrowded_2plus_per_room`, `pct_renter_overcrowded_2plus_per_room`.

* Household (3 variables): `avg_household_size`, `avg_family_size`, `pct_single_parent_households`.

* Demographic (12 variables): `median_age`, `pct_white_alone`, `pct_black_alone`, `pct_asian_alone`, `pct_two_or_more_races`, `pct_population_under_5`, `pct_population_5_to_17`, `pct_population_18_to_24`, `pct_population_25_to_44`, `pct_population_45_to_64`, `pct_population_65_plus`, `pct_female`.

::: {.callout-note}
## Variable list to be cleaned up and made more readable.
:::

The full variable list, including Census ACS table references and calculation methods, is specified in `census_specs.py`.

### Geographical crosswalk

Load profiles are identified by ZIP+4. Our demographic information from the 2023ACS and 2020 DHS is available at the Census block group level. To join the two, we use a crosswalk from commercial data firm: Melissa. That crosswalk matches every Zip+4 postal code in Illinois to the Census Block that it was associated with in 2023. From here we aggregate the Census Blocks to their Block Groups–allowing them to be associated with our demographic information. Thus, we are able to characterize Block Groups both demographically and by the usage data of their residents.

When a ZIP+4 maps to multiple block groups and crosswalk weights are unavailable, we enforce a deterministic 1-to-1 linkage by assigning each ZIP+4 to a single block group (selecting the smallest GEOID) to avoid double-counting household-day observations; this introduces potential geographic misclassification that we treat as a limitation.

### Clusters

We cluster the load profiles into clusters via k-means. We use Euclidean distance on normalized load profiles, to focus on load shape rather than overall levels. We are aiming for a small number of clusters (4–10) to aid in interpretation. We selected $k=4$ using a combination of quantitative diagnostics and interpretability considerations. Quantitatively, we compared candidate values of $k$ using (i) the silhouette score to assess separation and cohesion of clusters, and (ii) the within-cluster sum of squares (WCSS) “elbow” curve to identify diminishing returns in fit as $k$ increases. Substantively, we also inspected average normalized load shapes by cluster to ensure that the resulting patterns were distinct and interpretable, and that clusters were not degenerate (e.g., extremely small or poorly separated).

TK details on each cluster, and brief description of each shape.

We selected cluster 1 as the baseline (reference category) because it contains the largest share of household-day observations and exhibits the most stable and representative load-shape pattern among the clusters, providing a stable comparison point for the log-ratio regressions.

## Demographic predictors of load profile mix


We begin by aggregating load profile clusters at the block group level. If $j$ is the block group, and $q$ is the cluster ($q\in\{0,1,2,3\}$), then

$$
C_{jq} = \sum_{i\in \mathrm{bg}(j)} \mathrm{I}(c_i = q)
$$

is the count of household-day observations assigned to cluster $q$ in block group $j$, where $c_i$ denotes the cluster assignment for household-day observation $i$. Under this aggregation, a single household with multiple sampled days contributes multiple observations to the block group totals.

We further normalize these to $\pi_{jq} = \frac{C_{jq}}{\sum_q C_{jq}}$, the proportion of cluster assignments in each block group.

Our aim is to understand how block group level demographics predict the load cluster mix. Because $(\pi_{j0},\pi_{j1},\pi_{j2},\pi_{j3})$ are compositional proportions that must sum to 1, we use **multinomial logistic regression** to model the distribution of cluster assignments. This approach properly accounts for the compositional constraint, handles zero counts naturally through the multinomial likelihood, and provides statistically correct standard errors and hypothesis tests.

### Model specification

We model the counts $(C_{j0}, C_{j1}, C_{j2}, C_{j3})$ as following a multinomial distribution:

$$
(C_{j0}, C_{j1}, C_{j2}, C_{j3}) \sim \mathrm{Multinomial}\left(\sum_q C_{jq}; \pi_{j0}, \pi_{j1}, \pi_{j2}, \pi_{j3}\right)
$$

We fit a single unified multinomial logit model with **Cluster 2 as the baseline (reference) category**. This cluster contains the largest share of household-day observations and exhibits the most stable and representative load-shape pattern among the clusters, providing a stable comparison point for the log-ratio regressions. The model estimates coefficients for Clusters 0, 1, and 3 relative to this baseline, parameterizing the log-odds as:

$$
\log\left(\frac{\pi_{jq}}{\pi_{j2}}\right) = \beta_{0q} + \beta_{1q} X_{1j} + \beta_{2q} X_{2j} + \cdots + \beta_{pq} X_{pj}
$$

for $q \in \{0, 1, 3\}$ (i.e., all non-baseline clusters).

This parameterization ensures that:

- All predicted probabilities are positive: $\pi_{jq} > 0$
- Probabilities sum to 1: $\sum_q \pi_{jq} = 1$
- The baseline cluster probability is determined by the constraint: $\pi_{j2} = \frac{1}{1 + \sum_{q \in \{0,1,3\}} \exp(\beta_{0q} + \beta_{1q} X_{1j} + \cdots)}$

We estimate this model via **maximum likelihood** using the VGAM package in R, which implements iteratively reweighted least squares (IRLS) optimization. The multinomial likelihood naturally weights observations by their sample size—block groups with more household-day observations contribute more to the likelihood and thus have greater influence on parameter estimates.

### Why multinomial logit?

An alternative approach would be to compute empirical log-ratios $\log(C_{jq}/C_{j2})$ and model them using separate weighted ordinary least squares (OLS) regressions. However, this approach has several statistical problems:

1. **Zero counts:** Log-ratios are undefined when counts are zero, requiring arbitrary smoothing constants
2. **Dependent errors:** Log-ratios for different clusters share a common denominator and are thus correlated, violating OLS independence assumptions
3. **Heteroskedasticity:** The variance of log-ratios depends on unknown proportions, creating heteroskedasticity that weighted OLS only partially addresses
4. **Invalid predictions:** Predicted proportions may not sum to 1

Multinomial logit avoids all these issues by using the correct likelihood for multinomial count data.

### Predictor variables and rank deficiency

Our analysis includes 47 demographic predictor variables across five categories (spatial, economic, housing, household, and demographic characteristics). However, several of these variables form **compositional groups** where predictors sum to a constant (e.g., income brackets sum to 100% of households). This creates linear dependencies (rank deficiency) in the design matrix.

To handle this, we:

1. **Detect rank deficiency** via QR decomposition of the predictor matrix
2. **Drop redundant predictors** that are exact linear combinations of others
3. In practice, this reduces our predictor set from 47 to approximately **42 active predictors** (plus an intercept term)

The final model includes **43 terms per cluster equation** (42 predictors + 1 intercept), yielding **129 total coefficients** across the 3 non-baseline cluster equations.

::: {.callout-note}
## Variable Selection is still in progress.
:::

### Interpretation

Coefficients are interpreted on the **log-odds scale**: $\beta_{pq}$ is the expected change in $\log(\pi_{jq}/\pi_{j2})$ for a one-unit increase in predictor $X_{pj}$, holding other predictors constant.

Equivalently, $\exp(\beta_{pq})$ is the **multiplicative effect** on the odds ratio $\pi_{jq}/\pi_{j2}$ for a one-unit increase in $X_{pj}$. For example:

- If $\beta_{pq} = 0.02$ for median household income (in thousands of dollars), then a \$10k increase in income multiplies the odds of cluster $q$ vs. baseline by $\exp(0.02 \times 10) = e^{0.2} \approx 1.22$ (a 22% increase in odds).

To obtain **predicted probabilities** for a block group with covariates $X_j$, we use the softmax transformation:

$$
\hat{\pi}_{jq} = \begin{cases}
\frac{\exp(\hat{\beta}_{0q} + \hat{\beta}_{1q} X_{1j} + \cdots)}
{1 + \sum_{m \in \{0,1,3\}} \exp(\hat{\beta}_{0m} + \hat{\beta}_{1m} X_{1j} + \cdots)}
& \text{if } q \in \{0,1,3\} \\[1em]
\frac{1}
{1 + \sum_{m \in \{0,1,3\}} \exp(\hat{\beta}_{0m} + \hat{\beta}_{1m} X_{1j} + \cdots)}
& \text{if } q = 2 \text{ (baseline)}
\end{cases}
$$

These predicted probabilities automatically sum to 1 and are guaranteed to be positive.

### Model performance

Our final model achieves a **pseudo-R² of 0.567** (McFadden's R²), indicating that demographic predictors explain approximately 57% of the variation in cluster distributions across block groups. The model successfully converged using VGAM's IRLS optimizer, and all coefficients have valid standard errors for hypothesis testing.
