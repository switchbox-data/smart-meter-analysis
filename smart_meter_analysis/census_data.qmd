
Finalized code
```{python}
# Imports
import polars as pl
import cenpy as cen

# Census uses special codes for missing/suppressed data instead of nulls
CENSUS_MISSING_CODES = [-666666666, -999999999, -888888888, -555555555, -222222222]

FIPS_code = '17' #Illinois

# Helper Functions
def build_geoid(df: pl.DataFrame) -> pl.DataFrame:
    """Create 11-digit tract GEOID per Census standard: state(2)+county(3)+tract(6)."""
    return df.with_columns(
        pl.concat_str([
            pl.col("state").cast(pl.Utf8).str.zfill(2),
            pl.col("county").cast(pl.Utf8).str.zfill(3),
            pl.col("tract").cast(pl.Utf8).str.zfill(6),
        ]).alias("GEOID")
    )

def safe_percent(numer: pl.Expr, denom: pl.Expr) -> pl.Expr:
    """Calculate percentage with null safety for Census data edge cases."""
    return (
        pl.when(denom.is_not_null() & numer.is_not_null() & (denom > 0))
        .then((numer / denom) * 100.0)
        .otherwise(None)
    )

def clean_census_values(df: pl.DataFrame) -> pl.DataFrame:
    """Replace Census missing data codes with proper nulls for statistical validity."""
    numeric_cols = [
        col for col in df.columns 
        if df[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]
    ]
    
    return df.with_columns([
        pl.when(pl.col(col).is_in(CENSUS_MISSING_CODES))
          .then(None)
          .otherwise(pl.col(col))
          .alias(col)
        for col in numeric_cols
    ])

# Load ACS 5-Year Data (2023)
conn_acs = cen.remote.APIConnection("ACSDP5Y2023")

# Variable mappings from Census codes to readable names
acs_vars = {
    "DP02_0001E": "Total_Households",
    "DP02_0016E": "Avg_Household_Size",
    "DP02_0017E": "Avg_Family_Size",
    "DP02_0060E": "Less_9th_Grade",
    "DP02_0062E": "High_School_Dip",
    "DP02_0063E": "Some_College",
    "DP02_0064E": "Associates_Deg",
    "DP02_0065E": "Bachelors_Deg",
    "DP02_0066E": "Grad/Professional_Deg",
    "DP03_0052E": "Income_Less_10k",
    "DP03_0053E": "Income_10k_to_15k",
    "DP03_0054E": "Income_15k_to_25k",
    "DP03_0055E": "Income_25k_to_35k",
    "DP03_0056E": "Income_35k_to_50k",
    "DP03_0057E": "Income_50k_to_75k",
    "DP03_0058E": "Income_75k_to_100k",
    "DP03_0059E": "Income_100k_to_150k",
    "DP03_0060E": "Income_150k_to_200k",
    "DP03_0061E": "Income_200k_Plus",
    "DP03_0062E": "Median_Household_Income",
    "DP04_0001E": "Total_Housing_Units",
    "DP04_0002E": "Occupied_Housing_Units",
    "DP04_0003E": "Vacant_Housing_Units",
    "DP04_0017E": "Built_2020_After",
    "DP04_0018E": "Built_2010_2019",
    "DP04_0019E": "Built_2000_2009",
    "DP04_0020E": "Built_1990_1999",
    "DP04_0021E": "Built_1980_1989",
    "DP04_0022E": "Built_1970_1979",
    "DP04_0023E": "Built_1960_1969",
    "DP04_0024E": "Built_1950_1959",
    "DP04_0025E": "Built_1940_1949",
    "DP04_0026E": "Built_Before_1940",
    "DP04_0028E": "Rooms_1",
    "DP04_0029E": "Rooms_2",
    "DP04_0030E": "Rooms_3",
    "DP04_0031E": "Rooms_4",
    "DP04_0032E": "Rooms_5",
    "DP04_0033E": "Rooms_6",
    "DP04_0034E": "Rooms_7",
    "DP04_0035E": "Rooms_8",
    "DP04_0036E": "Rooms_9_Plus",
    "DP04_0046E": "Owner_Occupied",
    "DP04_0047E": "Renter_Occupied",
    "DP04_0063E": "Heat_Utility_Gas",
    "DP04_0065E": "Heat_Electric",
    "DP05_0006E": "Age_5_to_9",
    "DP05_0007E": "Age_10_to_14",
    "DP05_0008E": "Age_15_to_19",
    "DP05_0009E": "Age_20_to_24",
    "DP05_0010E": "Age_25_to_34",
    "DP05_0011E": "Age_35_to_44",
    "DP05_0012E": "Age_45_to_54",
    "DP05_0013E": "Age_55_to_59",
    "DP05_0014E": "Age_60_to_64",
    "DP05_0015E": "Age_65_to_74",
    "DP05_0016E": "Age_75_to_84",
    "DP05_0017E": "Age_85_Plus",
    "DP04_0081E": "Value_Less_50k",
    "DP04_0082E": "Value_50k_to_100k",
    "DP04_0083E": "Value_100k_to_150k",
    "DP04_0084E": "Value_150k_to_200k",
    "DP04_0085E": "Value_200k_to_300k",
    "DP04_0086E": "Value_300k_to_500k",
    "DP04_0087E": "Value_500k_to_1M",
    "DP04_0088E": "Value_1M_Plus",
}

# Query ACS data
acs_df = conn_acs.query(
    cols=["NAME"] + list(acs_vars.keys()),
    geo_unit="tract",
    geo_filter={"state": FIPS_code}
)

acs_df = pl.from_pandas(acs_df)
acs_df = build_geoid(acs_df)
acs_df = acs_df.rename(
    {k: v for k, v in acs_vars.items() if k in acs_df.columns}
)

# Load Decennial Census Urban/Rural Data (2020)
# Urban/rural designation requires decennial census, not available in ACS

conn_dhc = cen.remote.APIConnection('DECENNIALDHC2020')

dhc_vars = {
    'H2_002N': 'Urban_Housing_Units',
    'H2_003N': 'Rural_Housing_Units'
}

dhc_df = conn_dhc.query(
    cols=['NAME'] + list(dhc_vars.keys()),
    geo_unit='tract',
    geo_filter={'state': FIPS_code}
)

dhc_df = pl.from_pandas(dhc_df)
dhc_df = build_geoid(dhc_df)
dhc_df = dhc_df.rename(
    {k: v for k, v in dhc_vars.items() if k in dhc_df.columns}
)

# Calculate urban/rural metrics
dhc_df = dhc_df.with_columns([
    pl.col('Urban_Housing_Units').cast(pl.Float64),
    pl.col('Rural_Housing_Units').cast(pl.Float64)
]).with_columns([
    safe_percent(
        pl.col('Urban_Housing_Units'),
        pl.col('Urban_Housing_Units') + pl.col('Rural_Housing_Units')
    ).alias('Urban_Percent'),
    
    # Classify based on majority
    pl.when((pl.col('Urban_Housing_Units') + pl.col('Rural_Housing_Units')) == 0)
     .then(pl.lit('No Housing'))
     .when(pl.col('Urban_Housing_Units') > pl.col('Rural_Housing_Units'))
     .then(pl.lit('Urban'))
     .otherwise(pl.lit('Rural'))
     .alias('Urban_Rural_Classification')
])

# Merge Datasets
census_df = acs_df.join(
    dhc_df.select(['GEOID', 'Urban_Percent', 'Urban_Rural_Classification']),
    on='GEOID',
    how='left'
)

# Data Type Conversion and Cleaning
# Convert string numerics to float (Census API returns strings)

numeric_cols = [
    col for col in census_df.columns 
    if col not in ["NAME", "Urban_Rural_Classification"]
]

census_df = census_df.with_columns([
    pl.col(col).cast(pl.Float64, strict=False) 
    for col in numeric_cols 
    if census_df[col].dtype == pl.Utf8
])
# Replace Census missing codes with nulls
census_df = clean_census_values(census_df)
```

Possible improvements
1. Error handling
2. Config files for variables
3. Data validation