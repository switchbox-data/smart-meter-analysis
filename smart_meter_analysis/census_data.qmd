
Finalized code
```{python}
# Imports
import polars as pl
import cenpy as cen

# Census uses special codes for missing/suppressed data instead of nulls
CENSUS_MISSING_CODES = [-666666666, -999999999, -888888888, -555555555, -222222222]

FIPS_code = '17' #Illinois

# Helper Functions
def build_geoid(df: pl.DataFrame) -> pl.DataFrame:
    """Create 11-digit tract GEOID per Census standard: state(2)+county(3)+tract(6)."""
    return df.with_columns(
        pl.concat_str([
            pl.col("state").cast(pl.Utf8).str.zfill(2),
            pl.col("county").cast(pl.Utf8).str.zfill(3),
            pl.col("tract").cast(pl.Utf8).str.zfill(6),
        ]).alias("GEOID")
    )

def safe_percent(numer: pl.Expr, denom: pl.Expr) -> pl.Expr:
    """Calculate percentage with null safety for Census data edge cases."""
    return (
        pl.when(denom.is_not_null() & numer.is_not_null() & (denom > 0))
        .then((numer / denom) * 100.0)
        .otherwise(None)
    )

def clean_census_values(df: pl.DataFrame) -> pl.DataFrame:
    """Replace Census missing data codes with proper nulls for statistical validity."""
    numeric_cols = [
        col for col in df.columns 
        if df[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]
    ]
    
    return df.with_columns([
        pl.when(pl.col(col).is_in(CENSUS_MISSING_CODES))
          .then(None)
          .otherwise(pl.col(col))
          .alias(col)
        for col in numeric_cols
    ])

# Load ACS 5-Year Data (2023)
conn_acs = cen.remote.APIConnection("ACSDP5Y2023")

# Variable mappings from Census codes to readable names
acs_vars = {
    "DP02_0001E": "Total_Households",
    "DP02_0016E": "Avg_Household_Size",
    "DP02_0017E": "Avg_Family_Size",
    "DP02_0060E": "Less_9th_Grade",
    "DP02_0062E": "High_School_Dip",
    "DP02_0063E": "Some_College",
    "DP02_0064E": "Associates_Deg",
    "DP02_0065E": "Bachelors_Deg",
    "DP02_0066E": "Grad/Professional_Deg",
    "DP03_0052E": "Income_Less_10k",
    "DP03_0053E": "Income_10k_to_15k",
    "DP03_0054E": "Income_15k_to_25k",
    "DP03_0055E": "Income_25k_to_35k",
    "DP03_0056E": "Income_35k_to_50k",
    "DP03_0057E": "Income_50k_to_75k",
    "DP03_0058E": "Income_75k_to_100k",
    "DP03_0059E": "Income_100k_to_150k",
    "DP03_0060E": "Income_150k_to_200k",
    "DP03_0061E": "Income_200k_Plus",
    "DP03_0062E": "Median_Household_Income",
    "DP04_0001E": "Total_Housing_Units",
    "DP04_0002E": "Occupied_Housing_Units",
    "DP04_0003E": "Vacant_Housing_Units",
    "DP04_0017E": "Built_2020_After",
    "DP04_0018E": "Built_2010_2019",
    "DP04_0019E": "Built_2000_2009",
    "DP04_0020E": "Built_1990_1999",
    "DP04_0021E": "Built_1980_1989",
    "DP04_0022E": "Built_1970_1979",
    "DP04_0023E": "Built_1960_1969",
    "DP04_0024E": "Built_1950_1959",
    "DP04_0025E": "Built_1940_1949",
    "DP04_0026E": "Built_Before_1940",
    "DP04_0028E": "Rooms_1",
    "DP04_0029E": "Rooms_2",
    "DP04_0030E": "Rooms_3",
    "DP04_0031E": "Rooms_4",
    "DP04_0032E": "Rooms_5",
    "DP04_0033E": "Rooms_6",
    "DP04_0034E": "Rooms_7",
    "DP04_0035E": "Rooms_8",
    "DP04_0036E": "Rooms_9_Plus",
    "DP04_0046E": "Owner_Occupied",
    "DP04_0047E": "Renter_Occupied",
    "DP04_0062E": "Heat_Utility_Gas",
    "DP04_0064E": "Heat_Electric",
    "DP05_0006E": "Age_5_to_9",
    "DP05_0007E": "Age_10_to_14",
    "DP05_0008E": "Age_15_to_19",
    "DP05_0009E": "Age_20_to_24",
    "DP05_0010E": "Age_25_to_34",
    "DP05_0011E": "Age_35_to_44",
    "DP05_0012E": "Age_45_to_54",
    "DP05_0013E": "Age_55_to_59",
    "DP05_0014E": "Age_60_to_64",
    "DP05_0015E": "Age_65_to_74",
    "DP05_0016E": "Age_75_to_84",
    "DP05_0017E": "Age_85_Plus",
    "DP04_0081E": "Value_Less_50k",
    "DP04_0082E": "Value_50k_to_100k",
    "DP04_0083E": "Value_100k_to_150k",
    "DP04_0084E": "Value_150k_to_200k",
    "DP04_0085E": "Value_200k_to_300k",
    "DP04_0086E": "Value_300k_to_500k",
    "DP04_0087E": "Value_500k_to_1M",
    "DP04_0088E": "Value_1M_Plus",
}

# Query ACS data
acs_df = conn_acs.query(
    cols=["NAME"] + list(acs_vars.keys()),
    geo_unit="tract",
    geo_filter={"state": FIPS_code}
)

acs_df = pl.from_pandas(acs_df)
acs_df = build_geoid(acs_df)
acs_df = acs_df.rename(
    {k: v for k, v in acs_vars.items() if k in acs_df.columns}
)

# Load Decennial Census Urban/Rural Data (2020)
# Urban/rural designation requires decennial census, not available in ACS

conn_dhc = cen.remote.APIConnection('DECENNIALDHC2020')

dhc_vars = {
    'H2_002N': 'Urban_Housing_Units',
    'H2_003N': 'Rural_Housing_Units'
}

dhc_df = conn_dhc.query(
    cols=['NAME'] + list(dhc_vars.keys()),
    geo_unit='tract',
    geo_filter={'state': FIPS_code}
)

dhc_df = pl.from_pandas(dhc_df)
dhc_df = build_geoid(dhc_df)
dhc_df = dhc_df.rename(
    {k: v for k, v in dhc_vars.items() if k in dhc_df.columns}
)

# Calculate urban/rural metrics
dhc_df = dhc_df.with_columns([
    pl.col('Urban_Housing_Units').cast(pl.Float64),
    pl.col('Rural_Housing_Units').cast(pl.Float64)
]).with_columns([
    safe_percent(
        pl.col('Urban_Housing_Units'),
        pl.col('Urban_Housing_Units') + pl.col('Rural_Housing_Units')
    ).alias('Urban_Percent'),
    
    # Classify based on majority
    pl.when((pl.col('Urban_Housing_Units') + pl.col('Rural_Housing_Units')) == 0)
     .then(pl.lit('No Housing'))
     .when(pl.col('Urban_Housing_Units') > pl.col('Rural_Housing_Units'))
     .then(pl.lit('Urban'))
     .otherwise(pl.lit('Rural'))
     .alias('Urban_Rural_Classification')
])

# Merge Datasets
census_df = acs_df.join(
    dhc_df.select(['GEOID', 'Urban_Percent', 'Urban_Rural_Classification']),
    on='GEOID',
    how='left'
)

# Data Type Conversion and Cleaning
# Convert string numerics to float (Census API returns strings)

numeric_cols = [
    col for col in census_df.columns 
    if col not in ["NAME", "Urban_Rural_Classification"]
]

census_df = census_df.with_columns([
    pl.col(col).cast(pl.Float64, strict=False) 
    for col in numeric_cols 
    if census_df[col].dtype == pl.Utf8
])
# Replace Census missing codes with nulls
census_df = clean_census_values(census_df)
```

Possible improvements
1. Error handling
2. Config files for variables
3. Data validation

Tests
```{python}
# Test group 1: After completing acs_df_polars

# Test 1: Check what columns the API actually returned
print("=" * 50)
print("TEST 1: Column inspection")
print("=" * 50)
print("Columns from API:", acs_df_polars.columns[:10])  # First 10 columns
print("\nLooking for GEOID-related columns:")
geoid_cols = [col for col in acs_df_polars.columns if 'GEO' in col.upper()]
print(f"Found: {geoid_cols}")

# Test 2: Examine geographic identifiers
print("\n" + "=" * 50)
print("TEST 2: Geographic identifier format")
print("=" * 50)
sample = acs_df_polars.select(['state', 'county', 'tract']).head(5)
print(sample)
print("\nData types:")
for col in ['state', 'county', 'tract']:
    if col in acs_df_polars.columns:
        print(f"{col}: {acs_df_polars[col].dtype}")

# Test 3: Check if GEOID already exists and its format
print("\n" + "=" * 50)
print("TEST 3: GEOID validation")
print("=" * 50)
if 'GEOID' in acs_df_polars.columns:
    geoid_sample = acs_df_polars.select(['GEOID', 'NAME']).head(5)
    print(geoid_sample)
    print(f"\nGEOID length: {acs_df_polars['GEOID'].str.len_chars().unique()}")
    print(f"GEOID format check (should be 11 digits): {acs_df_polars['GEOID'].str.len_chars().unique().to_list() == [11]}")
else:
    print("No GEOID column found - build_geoid function IS necessary")

# Test 4: Verify data completeness
print("\n" + "=" * 50)
print("TEST 4: Data completeness")
print("=" * 50)
print(f"Total tracts loaded: {len(acs_df_polars)}")
print(f"Expected ~3100-3300 tracts for Illinois")
print(f"Data looks complete: {3100 <= len(acs_df_polars) <= 3300}")


# Test 5: Check for nulls in critical columns
print("\n" + "=" * 50)
print("TEST 5: Null value check")
print("=" * 50)
critical_cols = ['state', 'county', 'tract', 'NAME', 'Median household income']
for col in critical_cols:
    if col in acs_df_polars.columns:
        null_count = acs_df_polars[col].null_count()
        print(f"{col}: {null_count} nulls")

# Test 6: Validate renamed columns
print("\n" + "=" * 50)
print("TEST 6: Column renaming validation")
print("=" * 50)
original_codes = list(acs_vars_to_download.keys())[:5]
print("Original codes to check:", original_codes)
print("Still present (renaming failed):", [c for c in original_codes if c in acs_df_polars.columns])
renamed = ['Median household income', 'Average household size', 'Owner occupied']
print("Renamed columns present:", [c for c in renamed if c in acs_df_polars.columns])

# Test 7: Data type validation
print("\n" + "=" * 50)
print("TEST 7: Data types for numeric columns")
print("=" * 50)
numeric_cols = ['Median household income', 'Average household size', 'Total housing units']
for col in numeric_cols:
    if col in acs_df_polars.columns:
        dtype = acs_df_polars[col].dtype
        sample_values = acs_df_polars[col].drop_nulls().head(3).to_list()
        print(f"{col}: {dtype} - Sample: {sample_values}")

# Test 8: Specific GEOID format test (if it exists)
print("\n" + "=" * 50)
print("TEST 8: GEOID structure validation")
print("=" * 50)
if 'GEOID' in acs_df_polars.columns:
    test_geoid = acs_df_polars['GEOID'][0]
    print(f"Sample GEOID: {test_geoid}")
    print(f"Length: {len(test_geoid)}")
    print(f"Illinois prefix (should start with '17'): {test_geoid[:2] == '17'}")
    print(f"All digits: {test_geoid.isdigit() if isinstance(test_geoid, str) else 'Not a string'}")


# Test group 2: After creating census_df_final

# Test 1: Verify join didn't lose records
print("=" * 50)
print("TEST 1: Join integrity")
print("=" * 50)
print(f"ACS records: {len(acs_df_polars)}")
print(f"DHC records: {len(dhc_df_polars)}")
print(f"Final records: {len(census_df_final)}")
print(f"Records preserved: {len(census_df_final) == len(acs_df_polars)}")
if len(census_df_final) != len(acs_df_polars):
    print("WARNING: Join changed row count - investigate!")

# Test 2: Check GEOID matching
print("\n" + "=" * 50)
print("TEST 2: GEOID match validation")
print("=" * 50)

acs_geoids = set(acs_df_polars['GEOID'].unique().to_list()[:10])
dhc_geoids = set(dhc_df_polars['GEOID'].unique().to_list()[:10])
print(f"Sample ACS GEOIDs: {list(acs_geoids)[:3]}")
print(f"Sample DHC GEOIDs: {list(dhc_geoids)[:3]}")
print(f"Intersection: {len(acs_geoids.intersection(dhc_geoids))}/10 match")


# Check for null Urban_Rural_Classification (indicates join failure)
null_urban = census_df_final.filter(pl.col('Urban_Rural_Classification').is_null()).height
print(f"\nTracts with no urban/rural data: {null_urban}")
if null_urban > 100:
    print("WARNING: High number of unmatched tracts - GEOID format issue likely")

# Test 3: Data type validation
print("\n" + "=" * 50)
print("TEST 4: Numeric column conversion")
print("=" * 50)
numeric_cols_to_check = [
    'Median household income',
    'Average household size', 
    'Total housing units',
    'Urban_Percent'
]

for col in numeric_cols_to_check:
    if col in census_df_final.columns:
        dtype = census_df_final[col].dtype
        is_numeric = dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]
        non_null = census_df_final[col].drop_nulls().head(3).to_list()
        print(f"{col[:30]:30} | Type: {str(dtype):10} | Numeric: {is_numeric} | Sample: {non_null[:2]}")

# Test 4: Value ranges
print("\n" + "=" * 50)
print("TEST 5: Data value sanity checks")
print("=" * 50)

checks = [
    ('Urban_Percent', 0, 100, "Should be 0-100"),
    ('Average household size', 1, 10, "Should be 1-10 people"),
    ('Median household income', 0, 500000, "Should be $0-500k"),
]

for col, min_val, max_val, note in checks:
    if col in census_df_final.columns:
        actual_min = census_df_final[col].min()
        actual_max = census_df_final[col].max()
        in_range = (actual_min >= min_val if actual_min else True) and (actual_max <= max_val if actual_max else True)
        print(f"{col}: [{actual_min:.1f} - {actual_max:.1f}] | Expected [{min_val}-{max_val}] | Valid: {in_range}")
        if not in_range:
            print(f"  WARNING: {note}")

# INVESTIGATION: Census missing data codes

print("=" * 50)
print("INVESTIGATING NEGATIVE VALUES")
print("=" * 50)

# Check the raw data from API
print("Checking Average household size column:")
problematic = census_df_final.filter(pl.col('Average household size') < 0)
print(f"Rows with negative values: {len(problematic)}")
print(f"Unique negative values: {problematic['Average household size'].unique().to_list()}")

# Check multiple columns for this pattern
census_numeric_cols = [
    'Average household size',
    'Median household income',
    'Average family size',
    'Total housing units'
]

print("\nNegative value patterns across columns:")
for col in census_numeric_cols:
    if col in census_df_final.columns:
        negative_vals = census_df_final.filter(pl.col(col) < 0)[col].unique().to_list()
        if negative_vals:
            print(f"{col}: {negative_vals}")

# See actual examples
print("\nSample tracts with negative values:")
print(census_df_final.filter(pl.col('Average household size') < 0).select(['NAME', 'GEOID', 'Average household size', 'Median household income']).head())

CENSUS_MISSING_CODES = [
    -666666666,  # Commonly used for suppressed data
    -999999999,  # Another suppression code
    -888888888,  # No data
    -555555555,  # Not applicable
    -222222222,  # Not available
]

# Get all numeric columns
numeric_cols = [col for col in census_df_final.columns 
                if census_df_final[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32]]

# Replace missing codes with nulls
census_df_final_clean = census_df_final.with_columns([
    pl.when(pl.col(col).is_in(CENSUS_MISSING_CODES))
      .then(None)
      .otherwise(pl.col(col))
      .alias(col)
    for col in numeric_cols
])

# Verify the fix
print("\nAfter cleaning:")
print(f"Minimum Average household size: {census_df_final_clean['Average household size'].min()}")
print(f"Minimum Median income: {census_df_final_clean['Median household income'].min()}")
print(f"Nulls in Average household size: {census_df_final_clean['Average household size'].null_count()}")

# Test 5: Column count verification
print("\n" + "=" * 50)
print("TEST 7: Column completeness")
print("=" * 50)
acs_cols = len(acs_df_polars.columns)
dhc_unique_cols = len([c for c in dhc_df_polars.columns if c not in acs_df_polars.columns])
expected_cols = acs_cols + 2  # Urban_Percent and Urban_Rural_Classification
actual_cols = len(census_df_final_clean.columns)
print(f"ACS columns: {acs_cols}")
print(f"Added from DHC: {dhc_unique_cols} (should be ~2)")
print(f"Final columns: {actual_cols}")
print(f"Column count correct: {abs(actual_cols - expected_cols) <= 3}")
```